---
title: "AlexNet: Review and Implementation Using TF2"
date: 2020-07-23
tags: [implement alexnet, dropout, Local Response Normalization, Data Augmentation]
header:
  image: "/images/deeplearning_papers/AlexNet/Author/Alex_Krizhevsky.jpg"
excerpt: "implement alexnet, dropout, Local Response Normalization, Data Augmentation"
mathjax: "true"
---
# Outline
   **1.Introduction**

   **2.AlexNet Review**

   **3.AlexNet Implementation**

   **4.Results**

   **5.Conclusions**

   **6.References**
# 1.Introduction
  Alex Krizhevsky, Geoffrey Hinton and ilya Sutskever (University of Toronto) created a neural network architecture called **AlexNet** marked as a breakthrough in deep learning. AlexNet trained on 1.2 million high-resolution images in ImageNet LSVRC-2010 to classify 1000 distinct classes with 60 million parameters and 650,000 neurons. This architecture achieved 1st prize in ILSVRC 2012.

## The highlights of this papers
  - One of the pioneer paper in Deep Learning using CNN for image classifications (Prior to 2012, image classification is combination of feature engineer techniques and a classifier such as SVM).
  - Using Relu instead of Sigmoid or Tanh.
  - Using Local Response Normalization and Overlapping Pooling
  - Applying reducing overfitting techniques: \*dropout, data augmentation (translation & horizontal reflection, and altering the intensities of RGB channels in training images).
  - Train on 2 GPS in parallel.
  - There are 8 layers in AlexNet architecture including 5 convolutional neural networks and 3 fully connected layers.

## Note
  - Top-5 error rate: this is the error rate which score of correct label is not included in the five most highest score predicted classes.

# 2.AlexNet Review
## 2.1 Preprocessing data [5][6]
  - Dataset consists variable-resolution images, while the model requires constant input dimensionality. First of all, the images are down-sampled to a fixed resolution of 256x256. The shorted side is rescaled to the lenght of 256, and then cropping the middle 256x256 of the image. Finally, these pixels are subtracted by the mean value over the training set (trained the network on the centerd raw RGB values of the pixels).
  - [Image](https://www.learnopencv.com/wp-content/uploads/2018/05/AlexNet-Resize-Crop-Input.jpg)
  - From the cropped 256x256 images
### Note
  - If the input is grayscale, it is converted to an RGB image by generating 3 channels RGB which have the same value as in grayscale.
  - **The input dimension of image is 227x227 instead 0f 224x224 made mention in the paper.**

## 2.2 AlexNet Architecture
  - There are 8 layers: 5 convolutional + 3 fully-connected layers.
  - Local response normalization (LRN) layers  follow the 1st and 2nd convolutional layers.
  - Overlapping Max Pooling (OMP) follows local response normalization layer as well as the 5th convolutional layer.
  - All layers uses ReLu activation function, except the last one using 1000-way softmax.
  [Image](https://engmrk.com/wp-content/uploads/2018/10/AlexNet_Presentation.mp4)
  - **n: no of filters, F: kernels size, P: padding, S: stride.**

### First Convolutional Layer
  - Input shape = **227x227x3**
  - CONV: 96 kernels of size 11x11x3, stride is 4 (n=96, F=11, P=0, S=4) => 227x227x3 -> 55x55x96
  - LRN: 55x55x96 -> 55x55x96
  - OMP: pool size is 3, stride is 2 (n=96, F=3, P=0, S=2) => 55x55x96 -> 27x27x96

### Second Convolutional Layer
  - CONV: 256 kernels of size 5x5x96 (n=256, F=5, P=2, S=1) => 27x27x96 -> 27x27x256
  - LRN: 27x27x256 -> 27x27x256
  - OMP: pool size is 3, stride is 2 (n=256, F=3, P=0, S=2) => 27x27x256 -> 13x13x256

### Third Convolutional Layer
  - CONV: 384 kernels of size 3x3x256 (n=384, F=3, P=1, S=1) => 13x13x256 -> 13x13x384

### Fourth Convolutional Layer

  - CONV: 384 kernels of size 3x3x384 (n=384, F=3, P=1, S=1) => 13x13x384 -> 13x13x384
### Fifth Convolutional Layer

  - CONV: 256 kernels of size 3x3x384 (n=256, F=3, P=1, S=1) => 13x13x384 -> 13x13x256
  - OMP: pool size is 3, stride is 2 (n=256, F=3, P=0, S=2) => 13x13x256 -> 6x6x256

### First Fully-connected Layer
  - DENSE: units is 4096

### Second Fully-connected Layer
  - DENSE: units is 4096

### Third Fully-connected Layer
  - DENSE: units is 1000

### 2.2.1 ReLU Nonlinearity
  - The ReLU is also named non-saturating activation because the gradient will never be close to zero as long as the activation is positive. This means that training process will be faster compared with saturating activation such as sigmoid or tanh (gradient descent's weight update will be very small leading to **vanishing gradient problem**).
  - The formula of Relu is $$f(x)=max(0,x)$$
  - Advantages of using ReLU are:
    + Resolve vanish problem.
    + Faster training than sigmoid and tanh (better convergence performance) ( > 6 times than tanh in the paper).
    + More efficient in computation than sigmoid ($$(1+e^{-x})^{-x}$$) and tanh.
    [Image_compare](https://cdn-images-1.medium.com/freeze/max/1000/0*GVOhO4bIyiuJjPvS.png?q=20)  
    [Image_tanh](https://www.learnopencv.com/wp-content/uploads/2018/05/Tanh-300x238.png)
    [Image_Relu](https://www.learnopencv.com/wp-content/uploads/2018/05/ReLU-2-300x229.png)
### 2.2.2 Local Response Normalization
### 2.2.3 Overlapping Pooling
### Note
  - The depth of this architecture is important, if it is lack of one convolutional layer (< 1% model's parameters), this lead to inferior performance.

## 2.3 Reducing Overfitting
### 2.3.1 Data Augmentation
### 2.3.2 Dropout

# 3.AlexNet Implementation
## Preprocessing Images
## Data augmentation
## Layers
## Alexnet


# 4.Results

# 5.Conclusions

# 6.References
  1/ [AlexNet Implementation Using Keras](https://engmrk.com/alexnet-implementation-using-keras/?utm_campaign=News&utm_medium=Community&utm_source=DataCamp.com)
  2/ [AlexNet: Review and Implementation](https://mc.ai/alexnet-review-and-implementation/)
  3/ [Key Deep Learning Architectures: AlexNet](https://medium.com/@pechyonkin/key-deep-learning-architectures-alexnet-30bf607595f1)
  4/ [Difference between Local Response Normalization and Batch Normalization](https://towardsdatascience.com/difference-between-local-response-normalization-and-batch-normalization-272308c034ac#:~:text=LRN%20has%20multiple%20directions%20to,position%20across%20all%20the%20activations)
  5/ [AlexNet: A Brief Review](https://mc.ai/alexnet-a-brief-review/)
  6/ [Understanding AlexNet](https://www.learnopencv.com/understanding-alexnet/)
