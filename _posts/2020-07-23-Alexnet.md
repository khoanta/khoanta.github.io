---
title: "AlexNet: Review and Implementation Using TF2"
date: 2020-07-23
tags: [implement alexnet, dropout, Local Response Normalization, Data Augmentation]
header:
  image: "/images/deeplearning_papers/AlexNet/Author/Alex_Krizhevsky.jpg"
excerpt: "implement alexnet, dropout, Local Response Normalization, Data Augmentation"
mathjax: "true"
---
# Outline
   **1.Introduction**

   **2.AlexNet Review**

   **3.AlexNet Implementation**

   **4.Results**

   **5.Conclusions**

   **6.References**
# 1.Introduction
  Alex Krizhevsky, Geoffrey Hinton and ilya Sutskever (University of Toronto) created a neural network architecture called **AlexNet** marked as a breakthrough in deep learning. AlexNet trained on 1.2 million high-resolution images in ImageNet LSVRC-2010 to classify 1000 distinct classes with 60 million parameters and 650,000 neurons. This architecture achieved 1st prize in ILSVRC 2012.

## The highlights of this papers
  - One of the pioneer paper in Deep Learning using CNN for image classifications (Prior to 2012, image classification is combination of feature engineer techniques and a classifier such as SVM).
  - Using Relu instead of Sigmoid or Tanh.
  - Using Local Response Normalization and Overlapping Pooling
  - Applying reducing overfitting techniques: \*dropout, data augmentation (translation & horizontal reflection, and altering the intensities of RGB channels in training images).
  - Train on 2 GPS in parallel.
  - There are 8 layers in AlexNet architecture including 5 convolutional neural networks and 3 fully connected layers.

## Note
  - Top-5 error rate: this is the error rate which score of correct label is not included in the five most highest score predicted classes.

# 2.AlexNet Review
## 2.1 Preprocessing data [5][6]
  - Dataset consists variable-resolution images, while the model requires constant input dimensionality. First of all, the images are down-sampled to a fixed resolution of 256x256. The shorted side is rescaled to the lenght of 256, and then cropping the middle 256x256 of the image. Finally, these pixels are subtracted by the mean value over the training set (trained the network on the centerd raw RGB values of the pixels).
  - [Image](https://www.learnopencv.com/wp-content/uploads/2018/05/AlexNet-Resize-Crop-Input.jpg)
  - From the cropped 256x256 images
### Note
  - If the input is grayscale, it is converted to an RGB image by generating 3 channels RGB which have the same value as in grayscale.
  - **The input dimension of image is 227x227 instead 0f 224x224 made mention in the paper.**

## 2.2 AlexNet Architecture
  - There are 8 layers: 5 convolutional + 3 fully-connected layers.
  - Local response normalization (LRN) layers  follow the 1st and 2nd convolutional layers.
  - Overlapping Max Pooling (OMP) follows local response normalization layer as well as the 5th convolutional layer.
  - All layers uses ReLu activation function, except the last one using 1000-way softmax.
  [Image](https://engmrk.com/wp-content/uploads/2018/10/AlexNet_Presentation.mp4)
  - **n: no of filters, F: kernels size, P: padding, S: stride.**

### First Convolutional Layer
  - Input shape = **227x227x3**
  - CONV: 96 kernels of size 11x11x3, stride is 4 (n=96, F=11, P=0, S=4) => 227x227x3 -> 55x55x96
  - LRN: 55x55x96 -> 55x55x96
  - OMP: pool size is 3, stride is 2 (n=96, F=3, P=0, S=2) => 55x55x96 -> 27x27x96

### Second Convolutional Layer
  - CONV: 256 kernels of size 5x5x96 (n=256, F=5, P=2, S=1) => 27x27x96 -> 27x27x256
  - LRN: 27x27x256 -> 27x27x256
  - OMP: pool size is 3, stride is 2 (n=256, F=3, P=0, S=2) => 27x27x256 -> 13x13x256

### Third Convolutional Layer
  - CONV: 384 kernels of size 3x3x256 (n=384, F=3, P=1, S=1) => 13x13x256 -> 13x13x384

### Fourth Convolutional Layer

  - CONV: 384 kernels of size 3x3x384 (n=384, F=3, P=1, S=1) => 13x13x384 -> 13x13x384
### Fifth Convolutional Layer

  - CONV: 256 kernels of size 3x3x384 (n=256, F=3, P=1, S=1) => 13x13x384 -> 13x13x256
  - OMP: pool size is 3, stride is 2 (n=256, F=3, P=0, S=2) => 13x13x256 -> 6x6x256

### First Fully-connected Layer
  - DENSE: units is 4096

### Second Fully-connected Layer
  - DENSE: units is 4096

### Third Fully-connected Layer
  - DENSE: units is 1000

### 2.2.1 ReLU Nonlinearity
  - The ReLU is also named non-saturating activation because the gradient will never be close to zero as long as the activation is positive. This means that training process will be faster compared with saturating activation such as sigmoid or tanh (gradient descent's weight update will be very small leading to **vanishing gradient problem**).
  - The formula of Relu is $$f(x)=max(0,x)$$
  - Advantages of using ReLU are:
    + Input normalization is not necessary to avoid saturating problem.
    + Resolve vanish problem.
    + Faster training than sigmoid and tanh (better convergence performance) ( > 6 times than tanh in the paper).
    + More efficient in computation than sigmoid ($$(1+e^{-x})^{-1}$$) and tanh.
  - Disadvantages of using ReLu are:
    + Unbounded activation function -> it needs normalization to compensate this.
    [Image_compare](https://cdn-images-1.medium.com/freeze/max/1000/0*GVOhO4bIyiuJjPvS.png?q=20)  
    [Image_tanh](https://www.learnopencv.com/wp-content/uploads/2018/05/Tanh-300x238.png)
    [Image_Relu](https://www.learnopencv.com/wp-content/uploads/2018/05/ReLU-2-300x229.png)

### 2.2.2 Local Response Normalization
  - Normalization techniques is usually used to limit unbounded activation (such as ReLu) from increasing the output layer values (encourage lateral inhibition).
  - The main purpose is to apply [local contrast enhancement](https://www.cambridgeincolour.com/tutorials/local-contrast-enhancement.htm#:~:text=Local%20contrast%20enhancement%20attempts%20to,appearance%20of%20small%2Dscale%20edges.) to gain locally maximum pixels which is useful for the next layers.
  - There are two different types of LRN:
  [Image](https://miro.medium.com/max/1225/1*MFl0tPjwvc49HirAJZPhEA.png)
    * **Inter-Channel:** (used in this paper): neighborhood defined is **across the channel**, normalization is carried out in the depth dimension.
    [Image_formula](https://miro.medium.com/max/1225/1*JXGTZuvplcGpyE8DuP4B2w.png)
    * $$i$$ is position of output kernel, $$x,y$$ are coordinate of a specific pixel value want to normalize, $$a,b$$ are before and after normalization value in that position.
    * N is depth of the output filter.
    * Constant $$(k, \alpha, \beta, n)$$ are hyper parameters. $$k$$ is used to avoid singularities (division by zero), $$\alpha$$ normalization constant is used as scaling factor, $$\beta$$ is considered as contrast constant, and $$n$$ is how deep channels opted for normalization. **if $$(k=0, \alpha=1, \beta=1, n=N)$$, it become the standard normalization**
    * Picture below is an example of $$(k=0, \alpha=1, \beta=1, n=2)$$ while N=4
    [Image_example](https://miro.medium.com/max/1225/1*DmnOhSTIzn04sC0w1d3FPg.png)
    * **Intra-Channel:** neighborhood is pixels surrounding the interesting position on the same channel only.
    [Image_formula](https://miro.medium.com/max/1225/1*-19IMI2wJVDtaz4Uf4dRog.png)
  - **Batch Normalization (optional):** used to solve the problem called **Internal Covariate Shift** which occurs when there is a changing distribution of the hidden activation. In training, due to the lack of memory, dataset is divided into small batches to fit the capacity of the memory, and it lead to the training batch may come from different distribution. This issue (**Covariate shift**) slow down the training process because the model attempt to learn a certain distribution which is different in the next batch. Therefore, we should keep the batches inside of a certain distribution until it converges. Covariate Shift can be avoid by select images in different batches by randomly selecting images. However, there still exist a problem called Internal Covariate Shift which occurs in the hidden neurons. The distribution keeps on changing as training updates the training parameters. This wy Batch Normalization was invented to mitigate this. There are 2 step in BN:
    * First Step: normalize the whole batch $$\mathcal{B}$$ to zero mean and unit variance
      + Calculate mean of entire mini-batch
      + Calculate variance of entire mini-batch
      + Normalize by subtracting mean and dividing with variance
    * Second Step: use 2 trainable parameters ($$\gamma, \beta$$) to scale and shift the normalized output.
    * Algorithm of BN:
    [Image](https://miro.medium.com/max/709/1*Hiq-rLFGDpESpr8QNsJ1jg.png)

  - The normalization compute for each pixel across all the activation in a batch.
  [Image](https://miro.medium.com/max/1750/1*PgUwNzUYs2_Sp5nrPfSZ5g.jpeg)
  - The trainable parameters let the model make its own decision whether or not apply BN in a layer because there are several case withou having normalization can gain better results. When $$\gamma=\sqrt{variance}$$ and $$\beta=mean$$ the BN is deactivated.
  [image_change_dsit](https://miro.medium.com/max/1225/1*gUOjaIspVsz-PVxLDLQGQA.png)
    *
  - Comparison of LRN and BN:
  [Image_comparision](https://miro.medium.com/max/1225/1*J7rxGz1f_2YWjdcsvqNCNA.png)


### 2.2.3 Overlapping Pooling

### Note
  - The depth of this architecture is important, if it is lack of one convolutional layer (< 1% model's parameters), this lead to inferior performance.

## 2.3 Reducing Overfitting
### 2.3.1 Data Augmentation
### 2.3.2 Dropout

# 3.AlexNet Implementation
## Preprocessing Images
## Data augmentation
## Layers
## Alexnet


# 4.Results

# 5.Conclusions

# 6.References
  1/ [AlexNet Implementation Using Keras](https://engmrk.com/alexnet-implementation-using-keras/?utm_campaign=News&utm_medium=Community&utm_source=DataCamp.com)
  2/ [AlexNet: Review and Implementation](https://mc.ai/alexnet-review-and-implementation/)
  3/ [Key Deep Learning Architectures: AlexNet](https://medium.com/@pechyonkin/key-deep-learning-architectures-alexnet-30bf607595f1)
  4/ [Difference between Local Response Normalization and Batch Normalization](https://towardsdatascience.com/difference-between-local-response-normalization-and-batch-normalization-272308c034ac#:~:text=LRN%20has%20multiple%20directions%20to,position%20across%20all%20the%20activations)
  5/ [AlexNet: A Brief Review](https://mc.ai/alexnet-a-brief-review/)
  6/ [Understanding AlexNet](https://www.learnopencv.com/understanding-alexnet/)
  7/ [LOCAL CONTRAST ENHANCEMENT](https://www.cambridgeincolour.com/tutorials/local-contrast-enhancement.htm#:~:text=Local%20contrast%20enhancement%20attempts%20to,appearance%20of%20small%2Dscale%20edges.)
