---
title: "AlexNet: Review and Implementation Using TF2"
date: 2020-07-23
tags: [implement alexnet, dropout, Local Response Normalization, Data Augmentation]
header:
  image: "/images/deeplearning_papers/AlexNet/Author/Alex_Krizhevsky.jpg"
excerpt: "implement alexnet, dropout, Local Response Normalization, Data Augmentation"
mathjax: "true"
---
# Outline
   **1.Introduction**

   **2.AlexNet Review**

   **3.AlexNet Implementation**

   **4.Results**

   **5.Conclusions**

   **6.References**
# 1.Introduction
  Alex Krizhevsky, Geoffrey Hinton and ilya Sutskever (University of Toronto) created a neural network architecture called **AlexNet** marked as a breakthrough in deep learning. AlexNet trained on 1.2 million high-resolution images in ImageNet LSVRC-2010 to classify 1000 distinct classes with 60 million parameters and 650,000 neurons. This architecture achieved 1st prize in ILSVRC 2012.

## The highlights of this papers
  - One of the pioneer paper in Deep Learning using CNN for image classifications (Prior to 2012, image classification is combination of feature engineer techniques and a classifier such as SVM).
  - Using Relu instead of Sigmoid or Tanh.
  - Using Local Response Normalization and Overlapping Pooling
  - Applying reducing overfitting techniques: \*dropout, data augmentation (translation & horizontal reflection, and altering the intensities of RGB channels in training images).
  - Train on 2 GPS in parallel.
  - There are 8 layers in AlexNet architecture including 5 convolutional neural networks and 3 fully connected layers.

## Note
  - Top-5 error rate: this is the error rate which score of correct label is not included in the five most highest score predicted classes.

# 2.AlexNet Review
## 2.1 Preprocessing data [5][6]
  - Dataset consists variable-resolution images, while the model requires constant input dimensionality. First of all, the images are down-sampled to a fixed resolution of 256x256. The shorted side is rescaled to the lenght of 256, and then cropping the middle 256x256 of the image. Finally, these pixels are subtracted by the mean value over the training set (trained the network on the centerd raw RGB values of the pixels).
  - [Image](https://www.learnopencv.com/wp-content/uploads/2018/05/AlexNet-Resize-Crop-Input.jpg)
  - From the cropped 256x256 images
### Note
  - If the input is grayscale, it is converted to an RGB image by generating 3 channels RGB which have the same value as in grayscale.
  - **The input dimension of image is 227x227 instead 0f 224x224 made mention in the paper.**

## 2.2 AlexNet Architecture
  - There are 8 layers: 5 convolutional + 3 fully-connected layers.
  - Local response normalization (LRN) layers  follow the 1st and 2nd convolutional layers.
  - Overlapping Max Pooling (OMP) follows local response normalization layer as well as the 5th convolutional layer.
  - All layers uses ReLu activation function, except the last one using 1000-way softmax.
  [Image](https://engmrk.com/wp-content/uploads/2018/10/AlexNet_Presentation.mp4)
  - **n: no of filters, F: kernels size, P: padding, S: stride.**

### First Convolutional Layer
  - Input shape = **227x227x3**
  - CONV: 96 kernels of size 11x11x3, stride is 4 (n=96, F=11, P=0, S=4) => 227x227x3 -> 55x55x96
  - LRN: 55x55x96 -> 55x55x96
  - OMP: pool size is 3, stride is 2 (n=96, F=3, P=0, S=2) => 55x55x96 -> 27x27x96

### Second Convolutional Layer
  - CONV: 256 kernels of size 5x5x96 (n=256, F=5, P=2, S=1) => 27x27x96 -> 27x27x256
  - LRN: 27x27x256 -> 27x27x256
  - OMP: pool size is 3, stride is 2 (n=256, F=3, P=0, S=2) => 27x27x256 -> 13x13x256

### Third Convolutional Layer
  - CONV: 384 kernels of size 3x3x256 (n=384, F=3, P=1, S=1) => 13x13x256 -> 13x13x384

### Fourth Convolutional Layer

  - CONV: 384 kernels of size 3x3x384 (n=384, F=3, P=1, S=1) => 13x13x384 -> 13x13x384
### Fifth Convolutional Layer

  - CONV: 256 kernels of size 3x3x384 (n=256, F=3, P=1, S=1) => 13x13x384 -> 13x13x256
  - OMP: pool size is 3, stride is 2 (n=256, F=3, P=0, S=2) => 13x13x256 -> 6x6x256

### First Fully-connected Layer
  - DENSE: units is 4096

### Second Fully-connected Layer
  - DENSE: units is 4096

### Third Fully-connected Layer
  - DENSE: units is 1000

### 2.2.1 ReLU Nonlinearity
  - The ReLU is also named non-saturating activation because the gradient will never be close to zero as long as the activation is positive. This means that training process will be faster compared with saturating activation such as sigmoid or tanh (gradient descent's weight update will be very small leading to **vanishing gradient problem**).
  - The formula of Relu is $$f(x)=max(0,x)$$
  - Advantages of using ReLU are:
    + Input normalization is not necessary to avoid saturating problem.
    + Resolve vanish problem.
    + Faster training than sigmoid and tanh (better convergence performance) ( > 6 times than tanh in the paper).
    + More efficient in computation than sigmoid ($$(1+e^{-x})^{-1}$$) and tanh.
  - Disadvantages of using ReLu are:
    + Unbounded activation function -> it needs normalization to compensate this.
    [Image_compare](https://cdn-images-1.medium.com/freeze/max/1000/0*GVOhO4bIyiuJjPvS.png?q=20)  
    [Image_tanh](https://www.learnopencv.com/wp-content/uploads/2018/05/Tanh-300x238.png)
    [Image_Relu](https://www.learnopencv.com/wp-content/uploads/2018/05/ReLU-2-300x229.png)

### 2.2.2 Local Response Normalization
  - Normalization techniques is usually used to limit unbounded activation (such as ReLu) from increasing the output layer values (encourage lateral inhibition).
  - The main purpose is to apply [local contrast enhancement](https://www.cambridgeincolour.com/tutorials/local-contrast-enhancement.htm#:~:text=Local%20contrast%20enhancement%20attempts%20to,appearance%20of%20small%2Dscale%20edges.) to gain locally maximum pixels which is useful for the next layers.
  - There are two different types of LRN:
  [Image](https://miro.medium.com/max/1225/1*MFl0tPjwvc49HirAJZPhEA.png)
    * **Inter-Channel:** (used in this paper): neighborhood defined is **across the channel**, normalization is carried out in the depth dimension.
    [Image_formula](https://miro.medium.com/max/1225/1*JXGTZuvplcGpyE8DuP4B2w.png)
    * $$i$$ is position of output kernel, $$x,y$$ are coordinate of a specific pixel value want to normalize, $$a,b$$ are before and after normalization value in that position.
    * N is depth of the output filter.
    * Constant $$(k, \alpha, \beta, n)$$ are hyper parameters. $$k$$ is used to avoid singularities (division by zero), $$\alpha$$ normalization constant is used as scaling factor, $$\beta$$ is considered as contrast constant, and $$n$$ is how deep channels opted for normalization. **if $$(k=0, \alpha=1, \beta=1, n=N)$$, it become the standard normalization**
    * Picture below is an example of $$(k=0, \alpha=1, \beta=1, n=2)$$ while N=4
    [Image_example](https://miro.medium.com/max/1225/1*DmnOhSTIzn04sC0w1d3FPg.png)
    * **Intra-Channel:** neighborhood is pixels surrounding the interesting position on the same channel only.
    [Image_formula](https://miro.medium.com/max/1225/1*-19IMI2wJVDtaz4Uf4dRog.png)
  - **Batch Normalization (optional):** used to solve the problem called **Internal Covariate Shift** which occurs when there is a changing distribution of the hidden activation. In training, due to the lack of memory, dataset is divided into small batches to fit the capacity of the memory, and it lead to the training batch may come from different distribution. This issue (**Covariate shift**) slow down the training process because the model attempt to learn a certain distribution which is different in the next batch. Therefore, we should keep the batches inside of a certain distribution until it converges. Covariate Shift can be avoid by select images in different batches by randomly selecting images. However, there still exist a problem called Internal Covariate Shift which occurs in the hidden neurons. The distribution keeps on changing as training updates the training parameters. This wy Batch Normalization was invented to mitigate this. There are 2 step in BN:
    * First Step: normalize the whole batch $$\mathcal{B}$$ to zero mean and unit variance
      + Calculate mean of entire mini-batch
      + Calculate variance of entire mini-batch
      + Normalize by subtracting mean and dividing with variance
    * Second Step: use 2 trainable parameters ($$\gamma, \beta$$) to scale and shift the normalized output.
    * Algorithm of BN:
    [Image](https://miro.medium.com/max/709/1*Hiq-rLFGDpESpr8QNsJ1jg.png)

  - The normalization compute for each pixel across all the activation in a batch.
  [Image](https://miro.medium.com/max/1750/1*PgUwNzUYs2_Sp5nrPfSZ5g.jpeg)
  - The trainable parameters let the model make its own decision whether or not apply BN in a layer because there are several case withou having normalization can gain better results. When $$\gamma=\sqrt{variance}$$ and $$\beta=mean$$ the BN is deactivated.
  [image_change_dsit](https://miro.medium.com/max/1225/1*gUOjaIspVsz-PVxLDLQGQA.png)
    *
  - Comparison of LRN and BN:
  [Image_comparision](https://miro.medium.com/max/1225/1*J7rxGz1f_2YWjdcsvqNCNA.png)


### 2.2.3 Overlapping Pooling
  - Generally, Max Pooling used to downsample the width and height of the tensor, while keeping the important features. Overlapping Max Pooling is actual Max Pooling, except each adjacent windows (used to compute max value of neighbors in a specific position) will overlap each other.
  - This paper used max pooling layer with pool size is 3 and stride is 2.
  [Image_OMP](http://media5.datahacker.rs/2020/02/2-1-1024x416.jpg)
### Note
  - The depth of this architecture is important, if it is lack of one convolutional layer (< 1% model's parameters), this lead to inferior performance.

## 2.3 Reducing Overfitting
### 2.3.1 Data Augmentation
  In this paper, the authors implement 2 type of data augmentation to enlarge the dataset by transforming images without storing their on disk (this process generates transformed images on CPU, so it does not affect the training process implementedd on GPU).
  - First form of data augmentation: it uses translation and horizontal reflect techniques by extracting random 224x224 patches from 256x256 image (translation: the maximum patches are taken from 256x256 images are $$(256-224)*(256-224)=1024$$), and then applying horizontal reflect lead to $$1024*2=2048$$ images. Therefore the first form the data augmentation increase the training set by a factor of 2048. **At the test time** it take average prediction (made by softmax layer) from 10 patches from the original image (256x256x3) by utilizing translation (takes 5 patches (4 corners + 1 central patch)) and horizontal reflect (doubles the patches from translation).
  - Second form of data augmentation **(Fancy PCA image augmentation)**:
    this method adjusts color of image via PCA technique to augment data (PCA color augmentation) by adding a value (computed by using formula below) corresponding to each pixel in each channel (in this case 3 values for RGB images):

    $I_{xy} = \[I_{xy}^{R}, I_{xy}^{G}, I_{xy}^{B}\]$
    
    $\[p_{1}, p_{2}, p_{3}\]\[\alpha_{1}\lambda_{1}, \alpha_{2}\lambda_{2}, \alpha_{3}\lambda_{3}\]^{T}$
    
    Where $$I_{xy}$$ is image pixel, $$p_{i}$$ and $$\lambda_{i}$$ are eigenvector and eigenvalue of the covriance matrix (3*3) of the RGB value list of training images set. Moreover, $$\alpha$$ is random variable drawn from distribution which has mean zero and standar deviation 0.1
     
    * Step1 Prepare data for computing PCA: each pixel is a vector of 3 element (R,G,B), it means that this method turns the image maxtrix n\*m*\3 to list of RGB value (n\*m)\*3. "Specifically, we perform PCA on the set of RGB pixel values throughout the ImageNet training set"
    * Step2 Normalize the data before computing PCA: calulate the mean (vector of 3 value) of each channel on the the RGB matrix forming from image matrix, the substracting the RGB matrix by the mean.
    * Step3 Calculate the covariance matrix: shape of the matrix is 3*3 in this case because there are 3 features 
    * Step4 Calculate the eigenvector and eignevalue to choose the best 3 eigenvector: compute eigenvectors and their correspoding eignevalues, sort the eignevalues from high to low, then slicing the first 3 values.
    * Step5 Performing PCA: Transform data in step2 using eigenvectors matrix
    * Step6 Final step: find 3 random variables (which their distritubtion has zero mean and 0.1 standard deviation). Next, they are mutiplied with 3 eigenvalues, and then mutiplying matrix of eigenvector with these values to get the final result. Finally, add each of 3 results to their corresponding channels to implement augmentation.

    **=> This scheme reduces the top-1 error rate by over 1%.**

### 2.3.2 Dropout
    - One of the most efficient method to reduce error rate is to combine prediction of distinct models; however, it also leads to consume vast majority of time in training phase (model becomse more complex). Therefore, dropout was invented to address these problems but it still save time in training phase.
    - Every time an input is fed to the network, part of its output are forced to become to zero (they cannot contribute to the netowrk in both forward and back propagation). Therefore, it gives a sence like each inputs will be learned in differents network, but they share the same weights. This technique makse sure neurons cannot depend on specific neurons (each time is new subset of other neurons), they have to learn features of input.
    - At the test time all the neurons are used, but they will be muitpied by the (1 - dropout rate)     
   **=> In this paper, the dropout help to reudce overfitting, but it also requires two times the normal time to converge.**
# 3.AlexNet Implementation
## Preprocessing Images
## Data augmentation
## Layers
  **Locoal Response Layer**
  -> it like a normal algorithm
  1/ Try to implement with numpy
  2/ Convert to tf2
  **s**
## Alexnet


# 4.Results

# 5.Conclusions

# 6.References
  1/ [AlexNet Implementation Using Keras](https://engmrk.com/alexnet-implementation-using-keras/?utm_campaign=News&utm_medium=Community&utm_source=DataCamp.com)

  2/ [AlexNet: Review and Implementation](https://mc.ai/alexnet-review-and-implementation/)
  
  3/ [Key Deep Learning Architectures: AlexNet](https://medium.com/@pechyonkin/key-deep-learning-architectures-alexnet-30bf607595f1)
  
  4/ [Difference between Local Response Normalization and Batch Normalization](https://towardsdatascience.com/difference-between-local-response-normalization-and-batch-normalization-272308c034ac#:~:text=LRN%20has%20multiple%20directions%20to,position%20across%20all%20the%20activations)
  
  5/ [AlexNet: A Brief Review](https://mc.ai/alexnet-a-brief-review/)
  
  6/ [Understanding AlexNet](https://www.learnopencv.com/understanding-alexnet/)
  
  7/ [LOCAL CONTRAST ENHANCEMENT](https://www.cambridgeincolour.com/tutorials/local-contrast-enhancement.htm#:~:text=Local%20contrast%20enhancement%20attempts%20to,appearance%20of%20small%2Dscale%20edges.)
  
  8/ [TF Implementing AlexNet in TensorFlow 2.0](http://datahacker.rs/tf-alexnet/)
  
  9/ [ALexNet Github](https://github.com/paniabhisek/AlexNet)
  
  10/ [Best Practices for Preparing and Augmenting Image Data for CNNs](https://machinelearningmastery.com/best-practices-for-preparing-and-augmenting-image-data-for-convolutional-neural-networks/)
  
  11/[Fancy PCA (Data Augmentation) with Scikit-Image](https://deshanadesai.github.io/notes/Fancy-PCA-with-Scikit-Image)
  
  12/ [Fancy PCA](https://pixelatedbrian.github.io/2018-04-29-fancy_pca/)
  
  13/ [Implementing the PCA based data augmentation](https://gist.github.com/akemisetti/ecf156af292cd2a0e4eb330757f415d2)
  
  11111/ [Detect and Recognize Vehicle’s License Plate with Machine Learning and Python — Part 1: Detection License Plate with Wpod-Net](https://medium.com/@quangnhatnguyenle/detect-and-recognize-vehicles-license-plate-with-machine-learning-and-python-part-1-detection-795fda47e922)
