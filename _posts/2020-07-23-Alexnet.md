---
title: "AlexNet: Review and Implementation Using TF2"
date: 2020-07-23
tags: [implement alexnet, dropout, Local Response Normalization, Data Augmentation]
header:
  image: "/images/deeplearning_papers/AlexNet/Author/Alex_Krizhevsky.jpg"
excerpt: "implement alexnet, dropout, Local Response Normalization, Data Augmentation"
mathjax: "true"
---
# Outline
   **1.Introduction**

   **2.AlexNet Review**

   **3.AlexNet Implementation**

   **4.Results**

   **5.Conclusions**

   **6.References**
# 1.Introduction
  Alex Krizhevsky, Geoffrey Hinton and ilya Sutskever (University of Toronto) created a neural network architecture called **AlexNet** marked as a breakthrough in deep learning. AlexNet trained on 1.2 million high-resolution images in ImageNet LSVRC-2010 to classify 1000 distinct classes with 60 million parameters and 650,000 neurons. This architecture achieved 1st prize in ILSVRC 2012.

## The highlights of this papers
  - One of the pioneer paper in Deep Learning using CNN for image classifications (Prior to 2012, image classification is combination of feature engineer techniques and a classifier such as SVM).
  - Using Relu instead of Sigmoid or Tanh.
  - Using Local Response Normalization and Overlapping Pooling
  - Applying reducing overfitting techniques: \*dropout, data augmentation (translation & horizontal reflection, and altering the intensities of RGB channels in training images).
  - Train on 2 GPS in parallel.
  - There are 8 layers in AlexNet architecture including 5 convolutional neural networks and 3 fully connected layers.

## Note
  - Top-5 error rate: this is the error rate which score of correct label is not included in the five most highest score predicted classes.

# 2.AlexNet Review
  - The depth of this architecture is important, if it is lack of one convolutional layer (< 1% model's parameters), this lead to inferior performance.
  - Dataset consists variable-resolution images, while the model requires constant input dimensionality.  
    * First
    * Second
# 3.AlexNet Implementation

# 4.Results

# 5.Conclusions

# 6.References
